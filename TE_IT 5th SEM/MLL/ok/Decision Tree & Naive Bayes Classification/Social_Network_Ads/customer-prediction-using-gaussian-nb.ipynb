{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Prediction Model Analysis Report\n\n## Introduction\n\nIn this analysis, we aimed to build a predictive model to determine whether a customer will purchase a product based on certain demographic factors such as age and gender, as well as estimated salary. We used the Naive Bayes classification algorithm and explored different variants, including Gaussian Naive Bayes and Multinomial Naive Bayes.\n\n## Data Preprocessing\n\n### Data Loading\n\nWe started by importing the necessary libraries and loading the dataset, which was retrieved from the \"Social Network Ads\" dataset. The dataset contained information about customers' age, gender, estimated salary, and whether they made a purchase.\n\n### Data Cleaning\n\nWe performed basic data cleaning by checking for missing values, which, in this dataset, were found to be zero. No further cleaning was required.\n\n### Feature Selection\n\nWe selected the following features for our analysis:\n- Age\n- Gender\n- Estimated Salary\n\n### Data Encoding\n\nWe encoded the categorical variable \"Gender\" using Label Encoding to convert it into a numerical format suitable for machine learning algorithms.\n\n### Data Splitting\n\nWe split the dataset into training and testing sets, with a 65% training set and a 35% testing set.\n\n### Feature Scaling\n\nWe applied Standard Scaling to standardize the numerical features to ensure all features have the same scale.\n\n## Model Selection\n\nWe experimented with different Naive Bayes variants:\n\n1. **Gaussian Naive Bayes:** We applied Gaussian Naive Bayes, which assumes that the features follow a Gaussian distribution.\n\n2. **Multinomial Naive Bayes:** This variant is suitable for discrete data, but it gave us an error due to negative values in the dataset.\n\n## Model Evaluation\n\n### Evaluation Metrics\n\nFor model evaluation, we used the following metrics:\n- **Accuracy:** 88.57%\n- **Precision:** 0.86\n- **Recall:** 0.82\n- **F1 Score:** 0.84\n\nThese metrics provide insights into the performance of our models. Precision measures the ability of the model to correctly predict positive cases, recall measures the proportion of actual positive cases that were correctly predicted, and the F1 score is the harmonic mean of precision and recall.\n\n### Visualization\n\nWe visualized the model evaluation metrics using a bar plot:\n\n![Metrics Bar Plot](metrics_bar_plot.png)\n\n## Conclusion\n\nIn this analysis, we built and evaluated Naive Bayes models for customer prediction. Our models achieved good accuracy, precision, recall, and F1 score. However, further exploration and optimization can be performed to enhance model performance.\n\n## Future Work\n\n1. Feature Engineering: Explore additional features or perform feature engineering to improve model performance.\n2. Hyperparameter Tuning: Optimize hyperparameters to find the best model configuration.\n3. Model Comparison: Compare Naive Bayes with other classification algorithms to identify the most suitable model for this task.\n4. Deployment: Consider deploying the best-performing model for real-time customer prediction.\n\nThis analysis serves as a starting point for customer prediction and can be extended and refined for more accurate predictions.","metadata":{}},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{"_uuid":"98b1230b-9ab9-4799-b024-249f26875b5f","_cell_guid":"bbacc69a-1279-43b8-99bf-92a7f7a5b844","trusted":true}},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom matplotlib.colors import ListedColormap","metadata":{"_uuid":"6ea3d7d8-3a3a-4063-b224-09117125e8cb","_cell_guid":"18ca4de1-2ba9-4238-a8de-da994029129d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T05:26:25.044009Z","iopub.execute_input":"2023-09-18T05:26:25.044424Z","iopub.status.idle":"2023-09-18T05:26:25.051328Z","shell.execute_reply.started":"2023-09-18T05:26:25.044395Z","shell.execute_reply":"2023-09-18T05:26:25.050183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/social-network-ads/Social_Network_Ads.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.053968Z","iopub.execute_input":"2023-09-18T05:26:25.054329Z","iopub.status.idle":"2023-09-18T05:26:25.092976Z","shell.execute_reply.started":"2023-09-18T05:26:25.054300Z","shell.execute_reply":"2023-09-18T05:26:25.092101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n# Check for missing values","metadata":{}},{"cell_type":"code","source":"missing_values = dataset.isnull().sum()\nprint(\"Missing Values:\\n\", missing_values)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.094484Z","iopub.execute_input":"2023-09-18T05:26:25.095018Z","iopub.status.idle":"2023-09-18T05:26:25.101269Z","shell.execute_reply.started":"2023-09-18T05:26:25.094990Z","shell.execute_reply":"2023-09-18T05:26:25.100461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary statistics","metadata":{}},{"cell_type":"code","source":"data_summary = dataset.describe()\nprint(\"Data Summary:\\n\", data_summary)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.102442Z","iopub.execute_input":"2023-09-18T05:26:25.102924Z","iopub.status.idle":"2023-09-18T05:26:25.128800Z","shell.execute_reply.started":"2023-09-18T05:26:25.102896Z","shell.execute_reply":"2023-09-18T05:26:25.127642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding Data Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.subplot(2, 2, 1)\nplt.hist(dataset['Age'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Age Distribution')\n\nplt.subplot(2, 2, 2)\nplt.hist(dataset['EstimatedSalary'], bins=20, color='salmon', edgecolor='black')\nplt.title('Estimated Salary Distribution')\n\nplt.subplot(2, 2, 3)\nplt.bar(dataset['Gender'].unique(), dataset['Gender'].value_counts(), color=['pink', 'lightblue'])\nplt.title('Gender Distribution')\n\nplt.subplot(2, 2, 4)\nplt.bar(dataset['Purchased'].unique(), dataset['Purchased'].value_counts(), color=['lightgreen', 'coral'])\nplt.title('Purchase Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.131861Z","iopub.execute_input":"2023-09-18T05:26:25.132730Z","iopub.status.idle":"2023-09-18T05:26:25.904823Z","shell.execute_reply.started":"2023-09-18T05:26:25.132695Z","shell.execute_reply":"2023-09-18T05:26:25.903592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting features and target variable","metadata":{}},{"cell_type":"code","source":"X = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.906257Z","iopub.execute_input":"2023-09-18T05:26:25.906675Z","iopub.status.idle":"2023-09-18T05:26:25.912418Z","shell.execute_reply.started":"2023-09-18T05:26:25.906637Z","shell.execute_reply":"2023-09-18T05:26:25.911685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical Data","metadata":{}},{"cell_type":"code","source":"labelencoder_X = LabelEncoder()\nX[:, 0] = labelencoder_X.fit_transform(X[:, 0])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.913394Z","iopub.execute_input":"2023-09-18T05:26:25.913951Z","iopub.status.idle":"2023-09-18T05:26:25.927116Z","shell.execute_reply.started":"2023-09-18T05:26:25.913922Z","shell.execute_reply":"2023-09-18T05:26:25.926290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the dataset into training and testing sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.928270Z","iopub.execute_input":"2023-09-18T05:26:25.929149Z","iopub.status.idle":"2023-09-18T05:26:25.941818Z","shell.execute_reply.started":"2023-09-18T05:26:25.929114Z","shell.execute_reply":"2023-09-18T05:26:25.940866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling using Standardization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.943058Z","iopub.execute_input":"2023-09-18T05:26:25.943989Z","iopub.status.idle":"2023-09-18T05:26:25.956220Z","shell.execute_reply.started":"2023-09-18T05:26:25.943950Z","shell.execute_reply":"2023-09-18T05:26:25.955099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gaussian Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"classifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.957605Z","iopub.execute_input":"2023-09-18T05:26:25.958231Z","iopub.status.idle":"2023-09-18T05:26:25.969310Z","shell.execute_reply.started":"2023-09-18T05:26:25.958190Z","shell.execute_reply":"2023-09-18T05:26:25.968478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\ntry:\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\nexcept ZeroDivisionError:\n    precision, recall, f1 = 0, 0, 0\n\nprint('Confusion Matrix:\\n', cm)\nprint('Accuracy: {:.2f}%'.format(accuracy * 100))\nprint('Precision: {:.2f}'.format(precision))\nprint('Recall: {:.2f}'.format(recall))\nprint('F1 Score: {:.2f}'.format(f1))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.972762Z","iopub.execute_input":"2023-09-18T05:26:25.973447Z","iopub.status.idle":"2023-09-18T05:26:25.988692Z","shell.execute_reply.started":"2023-09-18T05:26:25.973407Z","shell.execute_reply":"2023-09-18T05:26:25.987598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Names of the metrics\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n\n# Corresponding values\nvalues = [accuracy, precision, recall, f1_score]\n\n# Create a bar plot\nplt.figure(figsize=(8, 6))\nplt.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\nplt.ylim(0, 1)  # Set the y-axis limit to the range of [0, 1] for percentages\nplt.xlabel('Metrics')\nplt.ylabel('Value')\nplt.title('Model Evaluation Metrics')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:28:03.042780Z","iopub.execute_input":"2023-09-18T05:28:03.043980Z","iopub.status.idle":"2023-09-18T05:28:03.424238Z","shell.execute_reply.started":"2023-09-18T05:28:03.043945Z","shell.execute_reply":"2023-09-18T05:28:03.422809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize decision boundary","metadata":{}},{"cell_type":"code","source":"def plot_decision_boundary(classifier, X_train, y_train):\n    h = .02\n    x_min, x_max = X_train[:, 0].min() - 0.1, X_train[:, 0].max() + 0.1\n    y_min, y_max = X_train[:, 1].min() - 0.1, X_train[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.bwr)\n    plt.xlabel('Age')\n    plt.ylabel('Estimated Salary')\n    plt.title('Gaussian Naive Bayes Decision Boundary')\n    plt.show()\n\nplot_decision_boundary(classifier, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:25.989978Z","iopub.execute_input":"2023-09-18T05:26:25.990580Z","iopub.status.idle":"2023-09-18T05:26:26.260652Z","shell.execute_reply.started":"2023-09-18T05:26:25.990528Z","shell.execute_reply":"2023-09-18T05:26:26.259615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction for a new customer","metadata":{}},{"cell_type":"code","source":"new_customer_age = 70\nnew_customer_salary = 80000\n\n# Standardizing the input features\nnew_customer_data = scaler.transform([[new_customer_age, new_customer_salary]])\n\n# Making the prediction\nnew_customer_prediction = classifier.predict(new_customer_data)\n\nif new_customer_prediction[0] == 1:\n    print('The new customer is likely to make a purchase.')\nelse:\n    print('The new customer is not likely to make a purchase.')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:26:26.261980Z","iopub.execute_input":"2023-09-18T05:26:26.262377Z","iopub.status.idle":"2023-09-18T05:26:26.270641Z","shell.execute_reply.started":"2023-09-18T05:26:26.262344Z","shell.execute_reply":"2023-09-18T05:26:26.269596Z"},"trusted":true},"execution_count":null,"outputs":[]}]}